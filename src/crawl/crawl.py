"""
Web Content Extraction Module

é«˜çº§ç½‘é¡µå†…å®¹æŠ“å–æ¨¡å—ï¼Œæä¾›æ™ºèƒ½åæ£€æµ‹ã€å»¶è¿Ÿæ§åˆ¶å’Œå¹¶å‘ç®¡ç†åŠŸèƒ½ã€‚

ä¸»è¦ç»„ä»¶:
========

WebContentExtractor
    æ ¸å¿ƒæå–å™¨ç±»ï¼Œæä¾›å®Œæ•´çš„ç½‘é¡µå†…å®¹æŠ“å–åŠŸèƒ½

DomainTracker  
    åŸŸåè¯·æ±‚è·Ÿè¸ªå™¨ï¼Œç®¡ç†åŒåŸŸåè¯·æ±‚çš„å»¶è¿Ÿæ§åˆ¶

ä¾¿æ·å‡½æ•°:
========

scrape_website_to_markdown(url, **kwargs) -> dict
    çˆ¬å–å•ä¸ªç½‘ç«™å¹¶è½¬æ¢ä¸ºMarkdownæ ¼å¼

scrape_multiple_websites(urls, **kwargs) -> dict  
    æ‰¹é‡çˆ¬å–å¤šä¸ªç½‘ç«™

quick_scrape(url, **kwargs) -> str
    å¿«é€Ÿçˆ¬å–ï¼Œåªè¿”å›å†…å®¹å­—ç¬¦ä¸²

scrape_sync(url, **kwargs) -> dict
    åŒæ­¥ç‰ˆæœ¬çš„çˆ¬å–å‡½æ•°

æ ¸å¿ƒç‰¹æ€§:
========

ğŸ¤– æ™ºèƒ½åæ£€æµ‹
- éšæœºUser-Agentè½®æ¢
- æµè§ˆå™¨å‚æ•°ä¼˜åŒ–
- è¯·æ±‚å¤´éšæœºåŒ–

â±ï¸ å¤šå±‚å»¶è¿Ÿæ§åˆ¶  
- å…¨å±€è¯·æ±‚å»¶è¿Ÿ
- åŒåŸŸåå»¶è¿Ÿ
- è‡ªå®šä¹‰å»¶è¿Ÿè§„åˆ™

ğŸ”„ æ™ºèƒ½é‡è¯•æœºåˆ¶
- æŒ‡æ•°é€€é¿é‡è¯•
- æ™ºèƒ½é”™è¯¯åˆ¤æ–­
- å¯é…ç½®é‡è¯•ç­–ç•¥

ğŸ¯ å¹¶å‘æ§åˆ¶
- å…¨å±€ä¿¡å·é‡é™åˆ¶
- åŸŸåçº§åˆ«ç®¡æ§
- é˜²æ­¢æœåŠ¡å™¨è¿‡è½½

ğŸ“ å†…å®¹ä¼˜åŒ–
- æ­£æ–‡æ™ºèƒ½æå–
- Markdownæ ¼å¼è½¬æ¢
- æ— ç”¨å…ƒç´ æ¸…ç†

å¿«é€Ÿå¼€å§‹:
========

åŸºç¡€ç”¨æ³•ï¼š
    import asyncio
    from src.crawl.crawl import scrape_website_to_markdown
    
    async def main():
        result = await scrape_website_to_markdown("https://example.com")
        if result["success"]:
            print(f"æ ‡é¢˜: {result['title']}")
            print(f"å†…å®¹: {result['content']}")
    
    asyncio.run(main())

æ‰¹é‡çˆ¬å–ï¼š
    async def batch_crawl():
        urls = ["https://site1.com", "https://site2.com"]
        results = await scrape_multiple_websites(
            urls,
            use_anti_detection=True,
            same_domain_min_delay=10.0
        )
        for url, result in results.items():
            print(f"{url}: {result['success']}")

è‡ªå®šä¹‰å»¶è¿Ÿè§„åˆ™ï¼š
    def custom_delay(url: str) -> Optional[dict]:
        if "slow-site.com" in url:
            return {"same_domain_min_delay": 30.0}
        return None
    
    result = await scrape_website_to_markdown(
        "https://slow-site.com/page",
        custom_delay_rule=custom_delay
    )

é«˜çº§é…ç½®ï¼š
    async with WebContentExtractor(
        use_anti_detection=True,
        same_domain_min_delay=15.0,
        global_max_concurrent=1,
        custom_delay_rule=my_custom_rule
    ) as extractor:
        result = await extractor.extract_main_content(url)

æ³¨æ„äº‹é¡¹:
========
- å»ºè®®åœ¨ç”Ÿäº§ç¯å¢ƒä¸­å¯ç”¨åæ£€æµ‹åŠŸèƒ½
- åŒåŸŸåå»¶è¿Ÿåº”æ ¹æ®ç›®æ ‡ç½‘ç«™çš„åçˆ¬ç­–ç•¥è°ƒæ•´
- å¹¶å‘æ•°ä¸å®œè¿‡é«˜ï¼Œé¿å…è§¦å‘é™åˆ¶
- è‡ªå®šä¹‰å»¶è¿Ÿè§„åˆ™åº”è€ƒè™‘ç½‘ç«™çš„å…·ä½“æƒ…å†µ

"""

import asyncio
import logging
import random
import re
import time
from collections import defaultdict
from typing import Any, Optional
from urllib.parse import urlparse

import backoff
from crawl4ai import AsyncWebCrawler

from src.crawl.anti_detect import AntiDetectionConfig

# è®¾ç½®æ—¥å¿—
logger = logging.getLogger(__name__)


class DomainTracker:
    """åŸŸåè¯·æ±‚è·Ÿè¸ªå™¨
    
    ç”¨äºè·Ÿè¸ªå’Œç®¡ç†å¯¹ä¸åŒåŸŸåçš„è¯·æ±‚æ—¶é—´å’Œé¢‘ç‡ï¼Œå®ç°åŒåŸŸåå»¶è¿Ÿæ§åˆ¶ã€‚
    è¿™æ˜¯WebContentExtractoråçˆ¬ç­–ç•¥çš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚
    
    åŠŸèƒ½ç‰¹æ€§:
    --------
    - è·Ÿè¸ªæ¯ä¸ªåŸŸåçš„ä¸Šæ¬¡è¯·æ±‚æ—¶é—´
    - ç»Ÿè®¡æ¯ä¸ªåŸŸåçš„æ€»è¯·æ±‚æ¬¡æ•°
    - è®¡ç®—åŒåŸŸåè¯·æ±‚é—´çš„å¿…è¦ç­‰å¾…æ—¶é—´
    - æä¾›è¯·æ±‚ç»Ÿè®¡å’Œæ—¥å¿—è®°å½•
    
    ä½¿ç”¨åœºæ™¯:
    --------
    - é¿å…å¯¹å•ä¸€åŸŸåè¿›è¡Œè¿‡äºé¢‘ç¹çš„è¯·æ±‚
    - å®ç°åŸºäºåŸŸåçš„æ™ºèƒ½å»¶è¿Ÿç­–ç•¥
    - é˜²æ­¢è§¦å‘ç½‘ç«™çš„åçˆ¬è™«æœºåˆ¶
    - æä¾›åŸŸåçº§åˆ«çš„è¯·æ±‚ç›‘æ§
    
    ç¤ºä¾‹:
    -----
        tracker = DomainTracker()
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦ç­‰å¾…
        wait_time = tracker.should_wait_for_domain("https://example.com/page1", 5.0)
        if wait_time > 0:
            await asyncio.sleep(wait_time)
        
        # æ›´æ–°è¯·æ±‚è®°å½•
        tracker.update_domain_request("https://example.com/page1")
    
    æ³¨æ„äº‹é¡¹:
    --------
    - åŸŸåæå–ä¼šä¿ç•™ç«¯å£å·å’Œåè®®
    - åŒä¸€åŸŸåçš„ä¸åŒé¡µé¢è¢«è§†ä¸ºåŒä¸€åŸŸå
    - ç»Ÿè®¡ä¿¡æ¯ä¼šåœ¨å¯¹è±¡ç”Ÿå‘½å‘¨æœŸå†…æŒç»­ç´¯ç§¯
    """

    def __init__(self):
        self.domain_last_request = defaultdict(float)
        self.domain_request_count = defaultdict(int)

    def get_domain(self, url: str) -> str:
        """ä»URLæå–åŸŸå"""
        try:
            parsed = urlparse(url)
            return f"{parsed.netloc}"
        except Exception:
            return url

    def should_wait_for_domain(
        self, url: str, same_domain_delay: float
    ) -> float:
        """æ£€æŸ¥æ˜¯å¦éœ€è¦ä¸ºç‰¹å®šåŸŸåç­‰å¾…ï¼Œè¿”å›éœ€è¦ç­‰å¾…çš„æ—¶é—´"""
        domain = self.get_domain(url)
        current_time = time.time()
        last_request_time = self.domain_last_request[domain]

        if last_request_time == 0:
            # ç¬¬ä¸€æ¬¡è¯·æ±‚è¯¥åŸŸå
            self.domain_last_request[domain] = current_time
            self.domain_request_count[domain] = 1
            return 0

        time_since_last = current_time - last_request_time

        # å¦‚æœè·ç¦»ä¸Šæ¬¡è¯·æ±‚è¯¥åŸŸåçš„æ—¶é—´ä¸å¤Ÿï¼Œéœ€è¦ç­‰å¾…
        if time_since_last < same_domain_delay:
            wait_time = same_domain_delay - time_since_last
            return wait_time

        return 0

    def update_domain_request(self, url: str):
        """æ›´æ–°åŸŸåè¯·æ±‚æ—¶é—´"""
        domain = self.get_domain(url)
        self.domain_last_request[domain] = time.time()
        self.domain_request_count[domain] += 1

        # è®°å½•è¯·æ±‚ç»Ÿè®¡
        count = self.domain_request_count[domain]
        if count <= 5:
            logger.info(f"åŸŸå {domain} ç¬¬ {count} æ¬¡è¯·æ±‚")
        elif count % 10 == 0:
            logger.info(f"åŸŸå {domain} å·²è¯·æ±‚ {count} æ¬¡")


class WebContentExtractor:
    """ç½‘é¡µå†…å®¹æå–å™¨ï¼Œä¸“æ³¨äºæ­£æ–‡å†…å®¹å¹¶è½¬æ¢ä¸ºMarkdown
    
    è¿™æ˜¯ä¸€ä¸ªé«˜çº§çš„ç½‘é¡µå†…å®¹æŠ“å–å·¥å…·ï¼Œæ”¯æŒåæ£€æµ‹ã€æ™ºèƒ½å»¶è¿Ÿã€å¹¶å‘æ§åˆ¶å’Œè‡ªå®šä¹‰å»¶è¿Ÿè§„åˆ™ã€‚
    ä½¿ç”¨ crawl4ai ä½œä¸ºåº•å±‚çˆ¬å–å¼•æ“ï¼Œæä¾›ç¨³å®šå¯é çš„ç½‘é¡µå†…å®¹æå–æœåŠ¡ã€‚
    
    ä¸»è¦ç‰¹æ€§:
    --------
    - ğŸ¤– æ™ºèƒ½åæ£€æµ‹ï¼šéšæœºUser-Agentã€è¯·æ±‚å¤´è½®æ¢ã€æµè§ˆå™¨å‚æ•°ä¼˜åŒ–
    - â±ï¸ å¤šå±‚å»¶è¿Ÿæ§åˆ¶ï¼šå…¨å±€å»¶è¿Ÿã€åŒåŸŸåå»¶è¿Ÿã€è‡ªå®šä¹‰å»¶è¿Ÿè§„åˆ™
    - ğŸ”„ æ™ºèƒ½é‡è¯•æœºåˆ¶ï¼šåŸºäºbackoffçš„æŒ‡æ•°é€€é¿é‡è¯•ï¼Œæ™ºèƒ½é”™è¯¯åˆ¤æ–­
    - ğŸ¯ å¹¶å‘æ§åˆ¶ï¼šå…¨å±€ä¿¡å·é‡æ§åˆ¶ï¼Œé¿å…è¿‡è½½ç›®æ ‡æœåŠ¡å™¨
    - ğŸ“ å†…å®¹ä¼˜åŒ–ï¼šè‡ªåŠ¨æå–æ­£æ–‡ã€è½¬æ¢Markdownã€æ¸…ç†æ— ç”¨å…ƒç´ 
    - ğŸ”§ é«˜åº¦å¯é…ç½®ï¼šä¸°å¯Œçš„å‚æ•°è®¾ç½®ï¼Œæ»¡è¶³ä¸åŒåœºæ™¯éœ€æ±‚
    
    å‚æ•°è¯´æ˜:
    --------
    use_anti_detection : bool, default=True
        æ˜¯å¦å¯ç”¨åæ£€æµ‹åŠŸèƒ½ã€‚å¯ç”¨åå°†ä½¿ç”¨éšæœºUser-Agentã€ä¼˜åŒ–æµè§ˆå™¨å‚æ•°ç­‰
        
    min_delay : float, default=1.0
        å…¨å±€è¯·æ±‚é—´çš„æœ€å°å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰
        
    max_delay : float, default=3.0
        å…¨å±€è¯·æ±‚é—´çš„æœ€å¤§å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰
        
    same_domain_min_delay : float, default=3.0
        åŒåŸŸåè¯·æ±‚é—´çš„æœ€å°å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé€šå¸¸æ¯”å…¨å±€å»¶è¿Ÿæ›´é•¿
        
    same_domain_max_delay : float, default=8.0
        åŒåŸŸåè¯·æ±‚é—´çš„æœ€å¤§å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰
        
    max_retries : int, default=3
        æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œä½¿ç”¨æŒ‡æ•°é€€é¿ç­–ç•¥
        
    global_max_concurrent : int, default=3
        å…¨å±€æœ€å¤§å¹¶å‘è¯·æ±‚æ•°ï¼Œå¯ç”¨åæ£€æµ‹æ—¶ä¼šè¢«é™åˆ¶ä¸º2
        
    custom_delay_rule : callable, optional
        è‡ªå®šä¹‰å»¶è¿Ÿè§„åˆ™å‡½æ•°ï¼Œæ¥æ”¶URLå‚æ•°ï¼Œè¿”å›å»¶è¿Ÿé…ç½®å­—å…¸
        
    è‡ªå®šä¹‰å»¶è¿Ÿè§„åˆ™:
    -------------
    custom_delay_rule å‡½æ•°åº”æ¥æ”¶ä¸€ä¸ªURLå­—ç¬¦ä¸²ï¼Œè¿”å›åŒ…å«å»¶è¿Ÿé…ç½®çš„å­—å…¸æˆ–Noneã€‚
    æ”¯æŒçš„é…ç½®é”®åŒ…æ‹¬ï¼š
    
    - min_delay: è¦†ç›–å…¨å±€æœ€å°å»¶è¿Ÿ
    - max_delay: è¦†ç›–å…¨å±€æœ€å¤§å»¶è¿Ÿ  
    - same_domain_min_delay: è¦†ç›–åŒåŸŸåæœ€å°å»¶è¿Ÿ
    - same_domain_max_delay: è¦†ç›–åŒåŸŸåæœ€å¤§å»¶è¿Ÿ
    
    ç¤ºä¾‹:
        def my_delay_rule(url: str) -> Optional[dict]:
            if "strict-site.com" in url:
                return {
                    "same_domain_min_delay": 30.0,
                    "same_domain_max_delay": 60.0
                }
            elif "api.fast-site.com" in url:
                return {
                    "min_delay": 0.5,
                    "max_delay": 1.0,
                    "same_domain_min_delay": 1.0,
                    "same_domain_max_delay": 2.0
                }
            return None  # ä½¿ç”¨é»˜è®¤é…ç½®
    
    ä½¿ç”¨ç¤ºä¾‹:
    --------
    åŸºç¡€ç”¨æ³•ï¼š
        async with WebContentExtractor() as extractor:
            result = await extractor.extract_main_content("https://example.com")
            if result["success"]:
                print(f"æ ‡é¢˜: {result['title']}")
                print(f"å†…å®¹: {result['content']}")
    
    é«˜çº§é…ç½®ï¼š
        async with WebContentExtractor(
            use_anti_detection=True,
            same_domain_min_delay=10.0,
            same_domain_max_delay=20.0,
            global_max_concurrent=1,
            custom_delay_rule=my_delay_rule
        ) as extractor:
            # æ‰¹é‡å¤„ç†å¤šä¸ªURL
            results = await extractor.extract_multiple_urls([
                "https://site1.com/page1",
                "https://site1.com/page2",  # ä¼šåº”ç”¨åŒåŸŸåå»¶è¿Ÿ
                "https://site2.com/page1"
            ])
    
    ä¾¿æ·å‡½æ•°ç”¨æ³•ï¼š
        # å•ä¸ªURL
        result = await scrape_website_to_markdown(
            "https://example.com",
            custom_delay_rule=my_delay_rule
        )
        
        # æ‰¹é‡URL
        results = await scrape_multiple_websites(
            ["https://site1.com", "https://site2.com"],
            custom_delay_rule=my_delay_rule
        )
    
    æ³¨æ„äº‹é¡¹:
    --------
    - å¯ç”¨åæ£€æµ‹æ¨¡å¼æ—¶ï¼Œå»ºè®®åŒåŸŸåå»¶è¿Ÿè‡³å°‘5ç§’ä»¥ä¸Š
    - è‡ªå®šä¹‰å»¶è¿Ÿè§„åˆ™çš„å¼‚å¸¸ä¼šè¢«æ•è·å¹¶å›é€€åˆ°é»˜è®¤é…ç½®
    - å¹¶å‘æ§åˆ¶æ˜¯å…¨å±€çš„ï¼Œä¼šå½±å“æ‰€æœ‰æ­£åœ¨è¿›è¡Œçš„è¯·æ±‚
    - é‡è¯•æœºåˆ¶ä¼šæ™ºèƒ½åˆ¤æ–­é”™è¯¯ç±»å‹ï¼Œé¿å…æ— æ„ä¹‰é‡è¯•
    """

    def __init__(
        self,
        use_anti_detection: bool = True,
        min_delay: float = 1.0,
        max_delay: float = 3.0,
        same_domain_min_delay: float = 3.0,
        same_domain_max_delay: float = 8.0,
        max_retries: int = 3,
        global_max_concurrent: int = 3,
        custom_delay_rule: Optional[callable] = None,
    ):
        self.crawler = None
        self.use_anti_detection = use_anti_detection
        self.min_delay = min_delay
        self.max_delay = max_delay
        self.same_domain_min_delay = same_domain_min_delay
        self.same_domain_max_delay = same_domain_max_delay
        self.max_retries = max_retries
        self.last_request_time = 0.0
        self.domain_tracker = DomainTracker()

        # å…¨å±€å¹¶å‘æ§åˆ¶
        self.global_max_concurrent = global_max_concurrent
        self.global_semaphore = None
        
        # è‡ªå®šä¹‰å»¶è¿Ÿè§„åˆ™
        self.custom_delay_rule = custom_delay_rule

    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        # åˆå§‹åŒ–å…¨å±€å¹¶å‘æ§åˆ¶ä¿¡å·é‡
        if self.use_anti_detection:
            # å¯ç”¨åæ£€æµ‹æ—¶ä½¿ç”¨æ›´ä¸¥æ ¼çš„å¹¶å‘æ§åˆ¶
            concurrent_limit = min(self.global_max_concurrent, 2)
        else:
            concurrent_limit = self.global_max_concurrent

        self.global_semaphore = asyncio.Semaphore(concurrent_limit)
        logger.info(f"åˆå§‹åŒ–å…¨å±€å¹¶å‘é™åˆ¶: {concurrent_limit}")

        # ä½¿ç”¨éšæœºUser-Agent
        user_agent = (
            AntiDetectionConfig.get_random_user_agent()
            if self.use_anti_detection
            else "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        )

        self.crawler = AsyncWebCrawler(
            verbose=True,
            headless=True,
            user_agent=user_agent,
            ignore_https_errors=True,
            java_script_enabled=True,
            load_images=False,  # ç¦ç”¨å›¾ç‰‡åŠ è½½
            block_resources=[
                "image",
                "media",
                "font",
                "stylesheet",
            ],  # é˜»æ­¢èµ„æºåŠ è½½
            # æ·»åŠ é¢å¤–çš„åæ£€æµ‹é…ç½®
            browser_args=(
                [
                    "--no-sandbox",
                    "--disable-blink-features=AutomationControlled",
                    "--disable-extensions",
                    "--disable-plugins",
                    "--disable-background-timer-throttling",
                    "--disable-backgrounding-occluded-windows",
                    "--disable-renderer-backgrounding",
                    "--disable-dev-shm-usage",
                ]
                if self.use_anti_detection
                else None
            ),
        )
        await self.crawler.__aenter__()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨é€€å‡º"""
        if self.crawler:
            await self.crawler.__aexit__(exc_type, exc_val, exc_tb)

    async def _apply_rate_limiting(self, url: str):
        """åº”ç”¨é€Ÿç‡é™åˆ¶ï¼ŒåŒ…æ‹¬åŒåŸŸåçš„ç‰¹æ®Šå¤„ç†"""
        if not self.use_anti_detection:
            return

        # è·å–å½“å‰URLçš„å»¶è¿Ÿé…ç½®ï¼ˆä¼˜å…ˆä½¿ç”¨è‡ªå®šä¹‰è§„åˆ™ï¼‰
        delay_config = self._get_delay_config_for_url(url)
        
        current_time = time.time()

        # 1. å…¨å±€è¯·æ±‚é—´éš”æ§åˆ¶
        time_since_last = current_time - self.last_request_time
        min_interval = random.uniform(delay_config["min_delay"], delay_config["max_delay"])
        if time_since_last < min_interval:
            wait_time = min_interval - time_since_last
            logger.debug(f"å…¨å±€å»¶è¿Ÿï¼šç­‰å¾… {wait_time:.2f} ç§’")
            await asyncio.sleep(wait_time)

        # 2. åŒåŸŸåå»¶è¿Ÿæ§åˆ¶ï¼ˆæ›´ä¸¥æ ¼ï¼‰
        same_domain_delay = random.uniform(
            delay_config["same_domain_min_delay"], 
            delay_config["same_domain_max_delay"]
        )
        domain_wait_time = self.domain_tracker.should_wait_for_domain(
            url, same_domain_delay
        )

        if domain_wait_time > 0:
            domain = self.domain_tracker.get_domain(url)
            logger.info(
                f"åŒåŸŸåå»¶è¿Ÿï¼š{domain} éœ€ç­‰å¾… {domain_wait_time:.2f} ç§’"
            )
            await asyncio.sleep(domain_wait_time)

        # æ›´æ–°è¯·æ±‚æ—¶é—´
        self.last_request_time = time.time()
        self.domain_tracker.update_domain_request(url)

    def _get_delay_config_for_url(self, url: str) -> dict:
        """è·å–æŒ‡å®šURLçš„å»¶è¿Ÿé…ç½®"""
        # é»˜è®¤é…ç½®
        default_config = {
            "min_delay": self.min_delay,
            "max_delay": self.max_delay,
            "same_domain_min_delay": self.same_domain_min_delay,
            "same_domain_max_delay": self.same_domain_max_delay,
        }
        
        # å¦‚æœæœ‰è‡ªå®šä¹‰è§„åˆ™ï¼Œå°è¯•è·å–è‡ªå®šä¹‰é…ç½®
        if self.custom_delay_rule:
            try:
                custom_config = self.custom_delay_rule(url)
                if custom_config and isinstance(custom_config, dict):
                    # åˆå¹¶é…ç½®ï¼Œè‡ªå®šä¹‰é…ç½®ä¼˜å…ˆ
                    result_config = default_config.copy()
                    result_config.update(custom_config)
                    
                    # è®°å½•ä½¿ç”¨äº†è‡ªå®šä¹‰é…ç½®
                    domain = self.domain_tracker.get_domain(url)
                    logger.debug(f"ä½¿ç”¨è‡ªå®šä¹‰å»¶è¿Ÿé…ç½®: {domain} - {custom_config}")
                    
                    return result_config
            except Exception as e:
                logger.warning(f"è‡ªå®šä¹‰å»¶è¿Ÿè§„åˆ™æ‰§è¡Œå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®: {e}")
        
        return default_config

    def _should_retry(self, exception: Exception) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥é‡è¯•"""
        # ç½‘ç»œç›¸å…³é”™è¯¯éœ€è¦é‡è¯•
        retry_exceptions = (
            ConnectionError,
            TimeoutError,
            OSError,
            asyncio.TimeoutError,
        )

        # æ£€æŸ¥å¼‚å¸¸ç±»å‹
        if isinstance(exception, retry_exceptions):
            return True

        # æ£€æŸ¥é”™è¯¯æ¶ˆæ¯ä¸­çš„å…³é”®è¯
        error_msg = str(exception).lower()
        retry_keywords = [
            "timeout",
            "connection",
            "network",
            "refused",
            "reset",
            "broken pipe",
            "temporary failure",
            "502 bad gateway",
            "503 service unavailable",
            "504 gateway timeout",
            "rate limit",
            "too many requests",
        ]

        return any(keyword in error_msg for keyword in retry_keywords)

    def _should_give_up(self, exception: Exception) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥æ”¾å¼ƒé‡è¯•"""
        # è¿™äº›é”™è¯¯ä¸åº”è¯¥é‡è¯•
        no_retry_exceptions = (
            PermissionError,
            ValueError,
            TypeError,
            KeyError,
            AttributeError,
        )

        if isinstance(exception, no_retry_exceptions):
            return True

        # æ£€æŸ¥é”™è¯¯æ¶ˆæ¯ä¸­çš„å…³é”®è¯
        error_msg = str(exception).lower()
        no_retry_keywords = [
            "404 not found",
            "401 unauthorized",
            "403 forbidden",
            "400 bad request",
            "file not found",
            "invalid url",
            "malformed url",
        ]

        return any(keyword in error_msg for keyword in no_retry_keywords)

    @backoff.on_exception(
        backoff.expo,
        Exception,  # æ•è·æ‰€æœ‰å¼‚å¸¸
        max_tries=4,  # æœ€å¤šé‡è¯•3æ¬¡ï¼ˆæ€»å…±4æ¬¡å°è¯•ï¼‰
        base=2,  # æŒ‡æ•°åº•æ•°
        factor=2,  # å»¶è¿Ÿå› å­
        max_value=30,  # æœ€å¤§å»¶è¿Ÿæ—¶é—´30ç§’
        giveup=lambda e: WebContentExtractor._should_give_up_static(
            e
        ),  # åˆ¤æ–­æ˜¯å¦æ”¾å¼ƒé‡è¯•
        on_backoff=lambda details: logger.warning(
            f"é‡è¯• {details['tries']}/{4}: {details['exception']}, "
            f"ç­‰å¾… {details['wait']:.1f}s åé‡è¯•"
        ),
        on_giveup=lambda details: logger.error(
            f"è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° {details['tries']}, æ”¾å¼ƒé‡è¯•: {details['exception']}"
        ),
        logger=logger,
        backoff_log_level=logging.WARNING,
        giveup_log_level=logging.ERROR,
    )
    async def _crawl_with_backoff(self, url: str, **crawl_config) -> Any:
        """ä½¿ç”¨ backoff è£…é¥°å™¨çš„çˆ¬å–æ–¹æ³•"""
        # ä½¿ç”¨å…¨å±€ä¿¡å·é‡æ§åˆ¶å¹¶å‘
        async with self.global_semaphore:
            # åº”ç”¨é€Ÿç‡é™åˆ¶ï¼ˆåŒ…æ‹¬åŸŸåé™åˆ¶ï¼‰
            await self._apply_rate_limiting(url)

            # å¦‚æœä½¿ç”¨åæ£€æµ‹ï¼Œæ·»åŠ éšæœºè¯·æ±‚å¤´
            if self.use_anti_detection:
                # æ³¨å…¥éšæœºheadersåˆ°crawlerä¸­
                # æ³¨æ„ï¼šcrawl4aiå¯èƒ½ä¸ç›´æ¥æ”¯æŒper-request headersï¼Œè¿™é‡Œä½œä¸ºç¤ºä¾‹
                pass

            # å°† URL æ·»åŠ åˆ°çˆ¬å–é…ç½®ä¸­
            crawl_config["url"] = url

            # æ‰§è¡Œçˆ¬å–
            result = await self.crawler.arun(**crawl_config)

            # æ£€æŸ¥ç»“æœæ˜¯å¦æˆåŠŸï¼Œå¤±è´¥åˆ™æŠ›å‡ºå¼‚å¸¸è§¦å‘é‡è¯•
            if not result.success:
                error_msg = result.error_message or "Unknown crawling error"
                exception = Exception(f"çˆ¬å–å¤±è´¥: {error_msg}")

                # åªæœ‰åœ¨åˆ¤æ–­åº”è¯¥é‡è¯•çš„æƒ…å†µä¸‹æ‰æŠ›å‡ºå¼‚å¸¸
                if self._should_retry(exception):
                    raise exception
                else:
                    # ä¸åº”è¯¥é‡è¯•çš„é”™è¯¯ï¼Œç›´æ¥è¿”å›å¤±è´¥ç»“æœ
                    logger.warning(
                        f"ä¸å¯é‡è¯•çš„é”™è¯¯ï¼Œç›´æ¥è¿”å›å¤±è´¥ç»“æœ: {error_msg}"
                    )
                    return result

            return result

    @staticmethod
    def _should_give_up_static(exception: Exception) -> bool:
        """é™æ€æ–¹æ³•ç‰ˆæœ¬çš„ _should_give_upï¼Œä¾› backoff è£…é¥°å™¨ä½¿ç”¨"""
        # è¿™äº›é”™è¯¯ä¸åº”è¯¥é‡è¯•
        no_retry_exceptions = (
            PermissionError,
            ValueError,
            TypeError,
            KeyError,
            AttributeError,
        )

        if isinstance(exception, no_retry_exceptions):
            return True

        # æ£€æŸ¥é”™è¯¯æ¶ˆæ¯ä¸­çš„å…³é”®è¯
        error_msg = str(exception).lower()
        no_retry_keywords = [
            "404 not found",
            "401 unauthorized",
            "403 forbidden",
            "400 bad request",
            "file not found",
            "invalid url",
            "malformed url",
        ]

        return any(keyword in error_msg for keyword in no_retry_keywords)

    def clean_markdown(self, markdown_content: str) -> str:
        """æ¸…ç†Markdownå†…å®¹ï¼Œç§»é™¤å›¾ç‰‡å’Œå¤šä½™ç©ºè¡Œ"""
        if not markdown_content:
            return ""

        # ç§»é™¤å›¾ç‰‡æ ‡è®° ![alt](url) å’Œ ![alt](url "title")
        markdown_content = re.sub(r"!\[.*?\]\(.*?\)", "", markdown_content)

        # ç§»é™¤HTMLå›¾ç‰‡æ ‡ç­¾
        markdown_content = re.sub(r"<img[^>]*>", "", markdown_content)

        # ç§»é™¤ç©ºçš„é“¾æ¥ï¼ˆå¯èƒ½æ˜¯å›¾ç‰‡é“¾æ¥ï¼‰
        markdown_content = re.sub(r"\[\]\([^)]*\)", "", markdown_content)

        # æ¸…ç†å¤šä½™çš„ç©ºè¡Œï¼ˆè¶…è¿‡2ä¸ªè¿ç»­ç©ºè¡Œå‹ç¼©ä¸º2ä¸ªï¼‰
        markdown_content = re.sub(r"\n\s*\n\s*\n+", "\n\n", markdown_content)

        # ç§»é™¤è¡Œé¦–è¡Œå°¾ç©ºç™½
        lines = []
        for line in markdown_content.split("\n"):
            lines.append(line.rstrip())

        # ç§»é™¤å¼€å¤´å’Œç»“å°¾çš„ç©ºè¡Œ
        while lines and not lines[0].strip():
            lines.pop(0)
        while lines and not lines[-1].strip():
            lines.pop()

        return "\n".join(lines)

    async def extract_main_content(
        self, url: str, use_readability: bool = True
    ) -> dict[str, Any]:
        """æå–ç½‘é¡µä¸»è¦å†…å®¹"""
        try:
            # çˆ¬å–é…ç½®
            crawl_config = {
                "word_count_threshold": 50,  # æœ€å°‘50ä¸ªè¯
                "only_text": False,  # ä¿ç•™ç»“æ„ç”¨äºmarkdownè½¬æ¢
                "bypass_cache": True,
                "remove_overlay_elements": True,  # ç§»é™¤å¼¹çª—ç­‰è¦†ç›–å…ƒç´ 
            }

            # å¦‚æœä½¿ç”¨readabilityç®—æ³•æå–æ­£æ–‡
            if use_readability:
                crawl_config["extraction_strategy"] = "readability"

            # ä½¿ç”¨backoffè£…é¥°çš„æ–¹æ³•æ‰§è¡Œçˆ¬å–
            result = await self._crawl_with_backoff(url, **crawl_config)

            if not result.success:
                logger.error(f"çˆ¬å–å¤±è´¥: {result.error_message}")
                return {
                    "success": False,
                    "error": result.error_message,
                    "content": None,
                    "title": None,
                    "url": url,
                }

            # è·å–markdownå†…å®¹
            markdown_content = (
                result.markdown.fit_markdown
                if hasattr(result, "markdown")
                else ""
            )

            # å¦‚æœæ²¡æœ‰markdownå†…å®¹ï¼Œå°è¯•ä»HTMLè½¬æ¢
            if not markdown_content and result.cleaned_html:
                markdown_content = self._html_to_markdown_simple(
                    result.cleaned_html
                )

            # æ¸…ç†markdownå†…å®¹
            clean_markdown = self.clean_markdown(markdown_content)

            # æå–æ ‡é¢˜
            title = self._extract_title(result)

            return {
                "success": True,
                "content": clean_markdown,
                "title": title,
                "url": url,
                "word_count": (
                    len(clean_markdown.split()) if clean_markdown else 0
                ),
                "extracted_at": (
                    result.extracted_at
                    if hasattr(result, "extracted_at")
                    else None
                ),
            }

        except Exception as e:
            logger.exception(f"æå–å†…å®¹æ—¶å‡ºé”™ {url}:")
            return {
                "success": False,
                "error": str(e),
                "content": None,
                "title": None,
                "url": url,
            }

    def _extract_title(self, result) -> Optional[str]:
        """æå–é¡µé¢æ ‡é¢˜"""
        # ä¼˜å…ˆä½¿ç”¨metadataä¸­çš„æ ‡é¢˜
        if hasattr(result, "metadata") and result.metadata:
            if "title" in result.metadata:
                return result.metadata["title"]
            if "og:title" in result.metadata:
                return result.metadata["og:title"]

        # ä»markdownå†…å®¹ä¸­æå–ç¬¬ä¸€ä¸ªæ ‡é¢˜
        if hasattr(result, "markdown") and result.markdown:
            title_match = re.search(
                r"^#\s+(.+)$", result.markdown, re.MULTILINE
            )
            if title_match:
                return title_match.group(1).strip()

        return None

    def _html_to_markdown_simple(self, html_content: str) -> str:
        """ç®€å•çš„HTMLåˆ°Markdownè½¬æ¢ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
        try:
            import html2text

            h = html2text.HTML2Text()
            h.ignore_links = False
            h.ignore_images = True  # å¿½ç•¥å›¾ç‰‡
            h.ignore_emphasis = False
            h.body_width = 0
            h.unicode_snob = True
            return h.handle(html_content)
        except ImportError:
            logger.warning("html2textæœªå®‰è£…ï¼Œè¿”å›æ¸…ç†åçš„HTML")
            # ç®€å•æ¸…ç†HTMLæ ‡ç­¾
            import re

            clean_text = re.sub(r"<[^>]+>", "", html_content)
            return clean_text

    def _group_urls_by_domain(self, urls: list) -> dict[str, list]:
        """æŒ‰åŸŸååˆ†ç»„URL"""
        domain_groups = defaultdict(list)
        for url in urls:
            domain = self.domain_tracker.get_domain(url)
            domain_groups[domain].append(url)
        return dict(domain_groups)

    async def extract_multiple_urls(
        self, urls: list, max_concurrent: int = 2
    ) -> dict[str, Any]:
        """æ‰¹é‡æå–å¤šä¸ªURLçš„å†…å®¹ï¼Œä¼˜åŒ–åŒåŸŸåå¤„ç†"""
        # æŒ‰åŸŸååˆ†ç»„
        domain_groups = self._group_urls_by_domain(urls)
        logger.info(
            f"å‘ç° {len(domain_groups)} ä¸ªä¸åŒåŸŸåï¼š{list(domain_groups.keys())}"
        )

        # ä¸ºåŒåŸŸåURLæ·»åŠ é¢å¤–å»¶è¿Ÿ
        processed_results = {}

        for domain, domain_urls in domain_groups.items():
            logger.info(f"å¼€å§‹å¤„ç†åŸŸå {domain} çš„ {len(domain_urls)} ä¸ªURL")

            # ç›´æ¥å¤„ç†æ‰€æœ‰URLï¼Œå…¨å±€å¹¶å‘æ§åˆ¶ç”± _crawl_with_backoff ä¸­çš„ä¿¡å·é‡è´Ÿè´£
            async def extract_single(url):
                return await self.extract_main_content(url)

            # å¤„ç†è¯¥åŸŸåä¸‹çš„æ‰€æœ‰URL
            tasks = [extract_single(url) for url in domain_urls]
            results = await asyncio.gather(*tasks, return_exceptions=True)

            # æ•´ç†è¯¥åŸŸåçš„ç»“æœ
            for i, result in enumerate(results):
                url = domain_urls[i]
                if isinstance(result, Exception):
                    processed_results[url] = {
                        "success": False,
                        "error": str(result),
                        "content": None,
                        "title": None,
                        "url": url,
                    }
                else:
                    processed_results[url] = result

            # åŸŸåä¹‹é—´çš„å»¶è¿Ÿ
            if domain != list(domain_groups.keys())[-1]:  # ä¸æ˜¯æœ€åä¸€ä¸ªåŸŸå
                inter_domain_delay = random.uniform(2.0, 5.0)
                logger.info(
                    f"åŸŸå {domain} å¤„ç†å®Œæˆï¼Œç­‰å¾… {inter_domain_delay:.2f} ç§’åå¤„ç†ä¸‹ä¸€ä¸ªåŸŸå"
                )
                await asyncio.sleep(inter_domain_delay)

        return processed_results


# ä½¿ç”¨ç¤ºä¾‹å‡½æ•°
async def scrape_website_to_markdown(
    url: str,
    use_readability: bool = True,
    use_anti_detection: bool = True,
    min_delay: float = 1.0,
    max_delay: float = 3.0,
    same_domain_min_delay: float = 3.0,
    same_domain_max_delay: float = 8.0,
    global_max_concurrent: int = 3,
    custom_delay_rule: Optional[callable] = None,
) -> dict[str, Any]:
    """çˆ¬å–å•ä¸ªç½‘ç«™å¹¶è½¬æ¢ä¸ºMarkdown
    
    Args:
        url: è¦çˆ¬å–çš„URL
        use_readability: æ˜¯å¦ä½¿ç”¨readabilityç®—æ³•æå–æ­£æ–‡
        use_anti_detection: æ˜¯å¦å¯ç”¨åæ£€æµ‹åŠŸèƒ½
        min_delay: å…¨å±€æœ€å°å»¶è¿Ÿï¼ˆç§’ï¼‰
        max_delay: å…¨å±€æœ€å¤§å»¶è¿Ÿï¼ˆç§’ï¼‰
        same_domain_min_delay: åŒåŸŸåæœ€å°å»¶è¿Ÿï¼ˆç§’ï¼‰
        same_domain_max_delay: åŒåŸŸåæœ€å¤§å»¶è¿Ÿï¼ˆç§’ï¼‰
        global_max_concurrent: å…¨å±€æœ€å¤§å¹¶å‘æ•°
        custom_delay_rule: è‡ªå®šä¹‰å»¶è¿Ÿè§„åˆ™å‡½æ•°ï¼Œæ¥æ”¶URLå‚æ•°ï¼Œè¿”å›å»¶è¿Ÿé…ç½®å­—å…¸
                          ä¾‹å¦‚: lambda url: {"min_delay": 2.0} if "example.com" in url else None
    
    Returns:
        åŒ…å«çˆ¬å–ç»“æœçš„å­—å…¸
    """
    async with WebContentExtractor(
        use_anti_detection=use_anti_detection,
        min_delay=min_delay,
        max_delay=max_delay,
        same_domain_min_delay=same_domain_min_delay,
        same_domain_max_delay=same_domain_max_delay,
        global_max_concurrent=global_max_concurrent,
        custom_delay_rule=custom_delay_rule,
    ) as extractor:
        result = await extractor.extract_main_content(url, use_readability)
        return result


async def scrape_multiple_websites(
    urls: list,
    max_concurrent: int = 2,
    use_anti_detection: bool = True,
    min_delay: float = 1.0,
    max_delay: float = 3.0,
    same_domain_min_delay: float = 5.0,
    same_domain_max_delay: float = 12.0,
    global_max_concurrent: int = 2,
    custom_delay_rule: Optional[callable] = None,
) -> dict[str, Any]:
    """æ‰¹é‡çˆ¬å–å¤šä¸ªç½‘ç«™
    
    Args:
        urls: è¦çˆ¬å–çš„URLåˆ—è¡¨
        max_concurrent: æœ€å¤§å¹¶å‘æ•°ï¼ˆå·²åºŸå¼ƒï¼Œç”±global_max_concurrentæ§åˆ¶ï¼‰
        use_anti_detection: æ˜¯å¦å¯ç”¨åæ£€æµ‹åŠŸèƒ½
        min_delay: å…¨å±€æœ€å°å»¶è¿Ÿï¼ˆç§’ï¼‰
        max_delay: å…¨å±€æœ€å¤§å»¶è¿Ÿï¼ˆç§’ï¼‰
        same_domain_min_delay: åŒåŸŸåæœ€å°å»¶è¿Ÿï¼ˆç§’ï¼‰
        same_domain_max_delay: åŒåŸŸåæœ€å¤§å»¶è¿Ÿï¼ˆç§’ï¼‰
        global_max_concurrent: å…¨å±€æœ€å¤§å¹¶å‘æ•°
        custom_delay_rule: è‡ªå®šä¹‰å»¶è¿Ÿè§„åˆ™å‡½æ•°ï¼Œæ¥æ”¶URLå‚æ•°ï¼Œè¿”å›å»¶è¿Ÿé…ç½®å­—å…¸
                          ä¾‹å¦‚: lambda url: {"same_domain_min_delay": 15.0} if "slow-site.com" in url else None
    
    Returns:
        åŒ…å«æ‰€æœ‰URLçˆ¬å–ç»“æœçš„å­—å…¸
    """
    async with WebContentExtractor(
        use_anti_detection=use_anti_detection,
        min_delay=min_delay,
        max_delay=max_delay,
        same_domain_min_delay=same_domain_min_delay,
        same_domain_max_delay=same_domain_max_delay,
        global_max_concurrent=global_max_concurrent,
        custom_delay_rule=custom_delay_rule,
    ) as extractor:
        results = await extractor.extract_multiple_urls(urls, max_concurrent)
        return results


# ç®€åŒ–ç‰ˆä½¿ç”¨æ¥å£
async def quick_scrape(
    url: str, 
    stealth_mode: bool = True, 
    custom_delay_rule: Optional[callable] = None
) -> str:
    """å¿«é€Ÿçˆ¬å–ï¼Œåªè¿”å›æ¸…ç†åçš„markdownå†…å®¹
    
    Args:
        url: è¦çˆ¬å–çš„URL
        stealth_mode: æ˜¯å¦å¯ç”¨éšèº«æ¨¡å¼ï¼ˆåæ£€æµ‹ï¼‰
        custom_delay_rule: è‡ªå®šä¹‰å»¶è¿Ÿè§„åˆ™å‡½æ•°
    
    Returns:
        æ¸…ç†åçš„markdownå†…å®¹å­—ç¬¦ä¸²
    """
    result = await scrape_website_to_markdown(
        url, 
        use_anti_detection=stealth_mode,
        custom_delay_rule=custom_delay_rule
    )
    if result["success"]:
        return result["content"]
    else:
        logger.error(f"çˆ¬å–å¤±è´¥: {result['error']}")
        return ""


# åŒæ­¥ç‰ˆæœ¬ï¼ˆå¦‚æœéœ€è¦ï¼‰
def scrape_sync(url: str, use_anti_detection: bool = True) -> dict[str, Any]:
    """åŒæ­¥ç‰ˆæœ¬çš„çˆ¬å–å‡½æ•°"""
    return asyncio.run(
        scrape_website_to_markdown(url, use_anti_detection=use_anti_detection)
    )


# ä¸»å‡½æ•°ç¤ºä¾‹
async def main():
    """ä¸»å‡½æ•°ç¤ºä¾‹"""
    # æµ‹è¯•URLåˆ—è¡¨ï¼ˆåŒ…å«åŒä¸€åŸŸåçš„å¤šä¸ªURLï¼‰
    test_urls = [
        "http://www.geekpark.net/news/349139",
        "http://www.geekpark.net/news/349152",
        "http://www.geekpark.net/news/349159",
        "https://example.com/page1",
        "https://example.com/page2",
        # æ·»åŠ æ›´å¤šURL
    ]

    # å®šä¹‰è‡ªå®šä¹‰å»¶è¿Ÿè§„åˆ™ç¤ºä¾‹
    def custom_delay_for_sites(url: str) -> Optional[dict]:
        """æ ¹æ®URLè‡ªå®šä¹‰å»¶è¿Ÿè§„åˆ™çš„ç¤ºä¾‹å‡½æ•°"""
        if "geekpark.net" in url:
            # å¯¹geekpark.netä½¿ç”¨æ›´é•¿çš„å»¶è¿Ÿ
            return {
                "same_domain_min_delay": 15.0,
                "same_domain_max_delay": 25.0,
                "min_delay": 3.0,
                "max_delay": 5.0,
            }
        elif "example.com" in url:
            # å¯¹example.comä½¿ç”¨è¾ƒçŸ­çš„å»¶è¿Ÿ
            return {
                "same_domain_min_delay": 2.0,
                "same_domain_max_delay": 4.0,
                "min_delay": 0.5,
                "max_delay": 1.5,
            }
        # å…¶ä»–ç½‘ç«™ä½¿ç”¨é»˜è®¤é…ç½®
        return None

    # å•ä¸ªURLçˆ¬å–
    print("=== å•ä¸ªURLçˆ¬å–ç¤ºä¾‹ï¼ˆå¯ç”¨åæ£€æµ‹ + backoffé‡è¯•ï¼‰ ===")
    single_result = await scrape_website_to_markdown(
        "http://www.geekpark.net/news/349159",
        use_anti_detection=True,
        min_delay=1.5,
        max_delay=3.0,
        same_domain_min_delay=4.0,
        same_domain_max_delay=8.0,
    )

    if single_result["success"]:
        print(f"æ ‡é¢˜: {single_result['title']}")
        print(f"å­—æ•°: {single_result['word_count']}")
        print(
            f"å†…å®¹é¢„è§ˆ: {single_result['content'][:200]}..."
            if single_result["content"]
            else "No content"
        )
    else:
        print(f"çˆ¬å–å¤±è´¥: {single_result['error']}")

    print(
        "\n=== æ‰¹é‡URLçˆ¬å–ç¤ºä¾‹ï¼ˆå¯ç”¨åŒåŸŸåå»¶è¿Ÿ + backoffé‡è¯• + å…¨å±€å¹¶å‘æ§åˆ¶ + è‡ªå®šä¹‰å»¶è¿Ÿè§„åˆ™ï¼‰ ==="
    )
    # æ‰¹é‡URLçˆ¬å– - ä½¿ç”¨è‡ªå®šä¹‰å»¶è¿Ÿè§„åˆ™
    batch_results = await scrape_multiple_websites(
        test_urls[:4],  # é™åˆ¶æµ‹è¯•æ•°é‡
        max_concurrent=1,  # è¿™ä¸ªå‚æ•°ç°åœ¨ä¸»è¦ç”¨äºè®°å½•ï¼Œå®é™…å¹¶å‘ç”± global_max_concurrent æ§åˆ¶
        use_anti_detection=True,
        min_delay=2.0,
        max_delay=4.0,
        same_domain_min_delay=6.0,  # åŒåŸŸåå»¶è¿Ÿæ›´é•¿
        same_domain_max_delay=15.0,
        global_max_concurrent=1,  # å…¨å±€æœ€å¤§å¹¶å‘æ•°ä¸º1ï¼Œç¡®ä¿ä¸¥æ ¼æ§åˆ¶
        custom_delay_rule=custom_delay_for_sites,  # ä½¿ç”¨è‡ªå®šä¹‰å»¶è¿Ÿè§„åˆ™
    )

    for url, result in batch_results.items():
        print(f"\nURL: {url}")
        if result["success"]:
            print(f"  æ ‡é¢˜: {result['title']}")
            print(f"  å­—æ•°: {result['word_count']}")
            content_preview = (
                result["content"][:100] + "..."
                if result["content"]
                else "No content"
            )
            print(f"  å†…å®¹é¢„è§ˆ: {content_preview}")
        else:
            print(f"  å¤±è´¥åŸå› : {result['error']}")

    print("\n=== å¿«é€Ÿçˆ¬å–ç¤ºä¾‹ï¼ˆéšèº«æ¨¡å¼ + backoffé‡è¯•ï¼‰ ===")
    # å¿«é€Ÿçˆ¬å–ï¼ˆåªè¦å†…å®¹ï¼‰
    quick_content = await quick_scrape("https://example.com", stealth_mode=True)
    content_preview = (
        quick_content[:100] + "..." if quick_content else "No content"
    )
    print(f"å¿«é€Ÿçˆ¬å–ç»“æœ: {content_preview}")

    print("\n=== è‡ªå®šä¹‰å»¶è¿Ÿè§„åˆ™é«˜çº§ç¤ºä¾‹ ===")
    # æ›´å¤æ‚çš„è‡ªå®šä¹‰å»¶è¿Ÿè§„åˆ™ç¤ºä¾‹
    def advanced_delay_rule(url: str) -> Optional[dict]:
        """æ›´å¤æ‚çš„è‡ªå®šä¹‰å»¶è¿Ÿè§„åˆ™ç¤ºä¾‹"""
        from urllib.parse import urlparse
        
        domain = urlparse(url).netloc
        
        # æ ¹æ®åŸŸåè®¾ç½®ä¸åŒçš„å»¶è¿Ÿç­–ç•¥
        delay_map = {
            # ä¸¥æ ¼çš„ç½‘ç«™
            "geekpark.net": {
                "same_domain_min_delay": 20.0,
                "same_domain_max_delay": 30.0,
                "min_delay": 5.0,
                "max_delay": 8.0,
            },
            # æ™®é€šç½‘ç«™
            "example.com": {
                "same_domain_min_delay": 3.0,
                "same_domain_max_delay": 6.0,
                "min_delay": 1.0,
                "max_delay": 2.0,
            },
        }
        
        # ç²¾ç¡®åŒ¹é…åŸŸå
        if domain in delay_map:
            return delay_map[domain]
            
        # æ¨¡ç³ŠåŒ¹é…ï¼ˆå¦‚å­åŸŸåï¼‰
        for pattern, config in delay_map.items():
            if pattern in domain:
                return config
                
        # é»˜è®¤é…ç½®ï¼šå¯¹æ‰€æœ‰å…¶ä»–ç½‘ç«™ä½¿ç”¨è¾ƒå¿«çš„å»¶è¿Ÿ
        return {
            "same_domain_min_delay": 1.0,
            "same_domain_max_delay": 3.0,
            "min_delay": 0.5,
            "max_delay": 1.0,
        }

    print("ä½¿ç”¨é«˜çº§è‡ªå®šä¹‰å»¶è¿Ÿè§„åˆ™çˆ¬å–...")
    advanced_result = await scrape_website_to_markdown(
        "http://www.geekpark.net/news/349139",
        use_anti_detection=True,
        custom_delay_rule=advanced_delay_rule,
    )
    
    if advanced_result["success"]:
        print(f"é«˜çº§è§„åˆ™çˆ¬å–æˆåŠŸ: {advanced_result['title']}")
    else:
        print(f"é«˜çº§è§„åˆ™çˆ¬å–å¤±è´¥: {advanced_result['error']}")


if __name__ == "__main__":
    # è¿è¡Œç¤ºä¾‹
    asyncio.run(main())
