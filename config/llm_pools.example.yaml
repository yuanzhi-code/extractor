# LLM 池配置示例 - 简化设计
# 直接使用 provider:model 作为模型标识，无需额外ID字段

providers:
  # SiliconFlow 提供商
  siliconflow:
    api_base: "https://api.siliconflow.cn/v1"
    api_key: "your-siliconflow-api-key-here"
    provider: "openai"  # 使用 OpenAI 兼容格式
    models:
      - model: "Qwen/Qwen3-7B-Instruct"
        weight: 1
      - model: "Qwen/Qwen3-14B-Instruct"
        weight: 2
      - model: "llama-3.1-8b-instruct"
        weight: 1

  # 另一个提供商，可以有相同的模型名
  huggingface:
    api_base: "https://api-inference.huggingface.co/v1"
    api_key: "your-huggingface-api-key-here"
    provider: "huggingface"
    models:
      - model: "Qwen/Qwen3-7B-Instruct"  # 与SiliconFlow相同的模型名
        weight: 1
      - model: "codellama/CodeLlama-7b-hf"
        weight: 1

  # DeepSeek 提供商
  deepseek:
    api_base: "https://api.deepseek.com/v1"
    api_key: "your-deepseek-api-key-here"
    provider: "deepseek"
    models:
      - model: "deepseek-chat"
        weight: 1

pools:
  # 演示混合使用不同提供商的相同模型
  mixed_qwen_pool:
    description: "混合不同提供商的 Qwen 模型"
    models: 
      - "siliconflow:Qwen/Qwen3-7B-Instruct"     # SiliconFlow的Qwen
      - "huggingface:Qwen/Qwen3-7B-Instruct"     # HuggingFace的Qwen
    load_balance_strategy: "least-busy"
    temperature: 0.1
    timeout: 30

  # 单提供商池
  fast_pool:
    description: "快速响应池"
    models: 
      - "siliconflow:Qwen/Qwen3-7B-Instruct"
      - "siliconflow:llama-3.1-8b-instruct"
    load_balance_strategy: "least-busy"
    temperature: 0.0
    timeout: 10

  # 高质量模型池
  quality_pool:
    description: "高质量模型池"
    models: 
      - "siliconflow:Qwen/Qwen3-14B-Instruct"
      - "deepseek:deepseek-chat"
    load_balance_strategy: "least-busy"
    temperature: 0.1
    timeout: 30

default_pool: "fast_pool"

nodes:
  tagger: "fast_pool"
  score: "quality_pool"
  extract: "mixed_qwen_pool"  # 使用混合池 